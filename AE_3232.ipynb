{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData:\n",
    "    def __init__(self, smoothing, horizon):\n",
    "        # load data and do seperate train and test set\n",
    "        X_density = glob.glob(\"./AEData_32/ac_density_*_smoothing_{}.npy\".format(smoothing))\n",
    "        X_complexity = glob.glob(\"./AEData_32/ev_density_*_smoothing_{}.npy\".format(smoothing))\n",
    "        Y = glob.glob(\"./AEData_32/ev_2class_density_*.npy\")\n",
    "        if len(X_density) != len(X_complexity):\n",
    "            print(\"Input Dimension Mismatch! Quit!\")\n",
    "            return\n",
    "\n",
    "        for idx in range(len(X_density)):\n",
    "            if idx == 0:\n",
    "                self.X_density = np.load(X_density[idx])[None, ...]\n",
    "                self.X_complexity = np.load(X_complexity[idx])[None, ...]\n",
    "                self.Y = np.load(Y[idx])[None, ...]\n",
    "            else:\n",
    "                self.X_density = np.concatenate((self.X_density, np.load(X_density[idx])[None, ...]), axis=0)\n",
    "                self.X_complexity = np.concatenate((self.X_complexity, np.load(X_complexity[idx])[None, ...]), axis=0)\n",
    "                self.Y = np.concatenate((self.Y, np.load(Y[idx])[None, ...]), axis=0)\n",
    "\n",
    "        # concatenate the raw data as X and Y\n",
    "        # self.X: 27x240x32x32x2\n",
    "        # self.Y: 27x240x32x32x1\n",
    "        self.X = np.concatenate((self.X_density[..., np.newaxis], self.X_complexity[..., np.newaxis]), axis=-1)\n",
    "        self.Y = self.Y[..., np.newaxis]\n",
    "\n",
    "        # shift data based on time horizon\n",
    "        shift = int(horizon / 60.0)\n",
    "        self.X, self.Y = self.X[:, :-shift, ...], self.Y[:, shift:, ...]\n",
    "\n",
    "        # scale the data for better training result, won't impact optimal\n",
    "        #self.X, self.Y = self.X*100, self.Y*100\n",
    "        self.Y = self.Y*100\n",
    "\n",
    "    def dataloader(self, bs, ratio):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(self.X, self.Y, test_size=ratio, shuffle=True)\n",
    "        X_train, X_test = torch.Tensor(X_train), torch.Tensor(X_test)\n",
    "        Y_train, Y_test = torch.Tensor(Y_train), torch.Tensor(Y_test)\n",
    "\n",
    "        trainloader = DataLoader(TensorDataset(X_train, Y_train), batch_size=bs, shuffle=True, num_workers=0)\n",
    "        testloader = DataLoader(TensorDataset(X_test, Y_test), batch_size=bs, shuffle=True, num_workers=0)\n",
    "        return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):  # input res=32, output res=32\n",
    "    def __init__(self, nf):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64, nc=2\n",
    "            nn.Conv2d(2, nf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(nf, nf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(nf * 2, nf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            # nn.Conv2d(nf * 4, nf * 2, 4, 2, 1, bias=False),\n",
    "            # nn.BatchNorm2d(nf * 2),\n",
    "            # nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(nf * 4, nf * 2, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nf * 2, nf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(nf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(nf * 8, nf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(nf * 4, nf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(nf * 2, 1, 4, 2, 1, bias=False),\n",
    "            # nn.BatchNorm2d(nf),\n",
    "            # nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            # nn.ConvTranspose2d(nf, 1, 4, 2, 1, bias=False),\n",
    "            # nn.Tanh()\n",
    "            nn.ReLU()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n",
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(2, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(64, 32, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(32, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "smoothing = 4\n",
    "horizon = 60\n",
    "batch_size = 1\n",
    "test_ratio = 0.1\n",
    "epochs = 5000\n",
    "nf = 16\n",
    "\n",
    "# save path\n",
    "save_path = './Epochs_{}_Horizon_{}_Smooth_{}'.format(epochs, horizon, smoothing)\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "        \n",
    "# load data\n",
    "trainloader, testloader = LoadData(smoothing, horizon).dataloader(batch_size, test_ratio)\n",
    "\n",
    "# load computational graph\n",
    "net = AutoEncoder(nf)\n",
    "if torch.cuda.is_available():\n",
    "    GPU = 1\n",
    "    print('Training on GPU')\n",
    "    net = net.cuda()\n",
    "print(net)\n",
    "\n",
    "# setup paramaters\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999), weight_decay=0.00001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5000 \t Mean Square Error Loss: 1.0667495567928298\n",
      "Epoch: 2/5000 \t Mean Square Error Loss: 1.05201905541839\n",
      "Epoch: 3/5000 \t Mean Square Error Loss: 1.0498746109806842\n",
      "Epoch: 4/5000 \t Mean Square Error Loss: 1.0492865091587207\n",
      "Epoch: 5/5000 \t Mean Square Error Loss: 1.0490466441070683\n",
      "Epoch: 6/5000 \t Mean Square Error Loss: 1.0490275307180492\n",
      "Epoch: 7/5000 \t Mean Square Error Loss: 1.0485716165359051\n",
      "Epoch: 8/5000 \t Mean Square Error Loss: 1.0487054521568648\n",
      "Epoch: 9/5000 \t Mean Square Error Loss: 1.0482567144737083\n",
      "Epoch: 10/5000 \t Mean Square Error Loss: 1.0478782035316883\n",
      "Epoch: 11/5000 \t Mean Square Error Loss: 1.0474130299300828\n",
      "Epoch: 12/5000 \t Mean Square Error Loss: 1.0466840297108413\n",
      "Epoch: 13/5000 \t Mean Square Error Loss: 1.045776933805713\n",
      "Epoch: 14/5000 \t Mean Square Error Loss: 1.0448388315144943\n",
      "Epoch: 15/5000 \t Mean Square Error Loss: 1.0429742426054247\n",
      "Epoch: 16/5000 \t Mean Square Error Loss: 1.0406219450499723\n",
      "Epoch: 17/5000 \t Mean Square Error Loss: 1.0368634706760547\n",
      "Epoch: 18/5000 \t Mean Square Error Loss: 1.0319352070157999\n",
      "Epoch: 19/5000 \t Mean Square Error Loss: 1.025620298904355\n",
      "Epoch: 20/5000 \t Mean Square Error Loss: 1.0167382252266217\n",
      "Epoch: 21/5000 \t Mean Square Error Loss: 1.0033889475227897\n",
      "Epoch: 22/5000 \t Mean Square Error Loss: 0.9921390501525111\n",
      "Epoch: 23/5000 \t Mean Square Error Loss: 0.9775057377675587\n",
      "Epoch: 24/5000 \t Mean Square Error Loss: 0.9644791800606699\n",
      "Epoch: 25/5000 \t Mean Square Error Loss: 0.951595734352846\n",
      "Epoch: 26/5000 \t Mean Square Error Loss: 0.9413381490747301\n",
      "Epoch: 27/5000 \t Mean Square Error Loss: 0.9310061103629267\n",
      "Epoch: 28/5000 \t Mean Square Error Loss: 0.9189452366848868\n",
      "Epoch: 29/5000 \t Mean Square Error Loss: 0.909471377169238\n",
      "Epoch: 30/5000 \t Mean Square Error Loss: 0.90091241453482\n",
      "Epoch: 31/5000 \t Mean Square Error Loss: 0.8886127112799609\n",
      "Epoch: 32/5000 \t Mean Square Error Loss: 0.8780644259193451\n",
      "Epoch: 33/5000 \t Mean Square Error Loss: 0.8695958817853091\n",
      "Epoch: 34/5000 \t Mean Square Error Loss: 0.8608614889647671\n",
      "Epoch: 35/5000 \t Mean Square Error Loss: 0.850343797994957\n",
      "Epoch: 36/5000 \t Mean Square Error Loss: 0.844575778211011\n",
      "Epoch: 37/5000 \t Mean Square Error Loss: 0.8336270513893671\n",
      "Epoch: 38/5000 \t Mean Square Error Loss: 0.8261478565726816\n",
      "Epoch: 39/5000 \t Mean Square Error Loss: 0.8170860821233136\n",
      "Epoch: 40/5000 \t Mean Square Error Loss: 0.8103306273536205\n",
      "Epoch: 41/5000 \t Mean Square Error Loss: 0.803443453800728\n",
      "Epoch: 42/5000 \t Mean Square Error Loss: 0.7965628033402574\n",
      "Epoch: 43/5000 \t Mean Square Error Loss: 0.7899835518713278\n",
      "Epoch: 44/5000 \t Mean Square Error Loss: 0.782951341014527\n",
      "Epoch: 45/5000 \t Mean Square Error Loss: 0.7733207736554008\n",
      "Epoch: 46/5000 \t Mean Square Error Loss: 0.7633325963838332\n",
      "Epoch: 47/5000 \t Mean Square Error Loss: 0.754394178111184\n",
      "Epoch: 48/5000 \t Mean Square Error Loss: 0.7472545502076089\n",
      "Epoch: 49/5000 \t Mean Square Error Loss: 0.7392767203901602\n",
      "Epoch: 50/5000 \t Mean Square Error Loss: 0.7323988860621112\n",
      "Epoch: 51/5000 \t Mean Square Error Loss: 0.7246316776116025\n",
      "Epoch: 52/5000 \t Mean Square Error Loss: 0.7141039471247206\n",
      "Epoch: 53/5000 \t Mean Square Error Loss: 0.7078719378515268\n",
      "Epoch: 54/5000 \t Mean Square Error Loss: 0.6992830152790916\n",
      "Epoch: 55/5000 \t Mean Square Error Loss: 0.6919391594172521\n",
      "Epoch: 56/5000 \t Mean Square Error Loss: 0.6811887360018167\n",
      "Epoch: 57/5000 \t Mean Square Error Loss: 0.6727376153778332\n",
      "Epoch: 58/5000 \t Mean Square Error Loss: 0.663639669139016\n",
      "Epoch: 59/5000 \t Mean Square Error Loss: 0.6558575241136751\n",
      "Epoch: 60/5000 \t Mean Square Error Loss: 0.6473691004589511\n",
      "Epoch: 61/5000 \t Mean Square Error Loss: 0.6407366098220376\n",
      "Epoch: 62/5000 \t Mean Square Error Loss: 0.6350876077947258\n",
      "Epoch: 63/5000 \t Mean Square Error Loss: 0.6264950520822692\n",
      "Epoch: 64/5000 \t Mean Square Error Loss: 0.6165766556392652\n",
      "Epoch: 65/5000 \t Mean Square Error Loss: 0.6086305464660771\n",
      "Epoch: 66/5000 \t Mean Square Error Loss: 0.5995343080624377\n",
      "Epoch: 67/5000 \t Mean Square Error Loss: 0.5927410055902712\n",
      "Epoch: 68/5000 \t Mean Square Error Loss: 0.5840003021591379\n",
      "Epoch: 69/5000 \t Mean Square Error Loss: 0.5783504063115459\n",
      "Epoch: 70/5000 \t Mean Square Error Loss: 0.5716253853243265\n",
      "Epoch: 71/5000 \t Mean Square Error Loss: 0.5638907494405323\n",
      "Epoch: 72/5000 \t Mean Square Error Loss: 0.556268855617635\n",
      "Epoch: 73/5000 \t Mean Square Error Loss: 0.5502841861676971\n",
      "Epoch: 74/5000 \t Mean Square Error Loss: 0.5454366715882113\n",
      "Epoch: 75/5000 \t Mean Square Error Loss: 0.5400967677766807\n",
      "Epoch: 76/5000 \t Mean Square Error Loss: 0.5328191154671512\n",
      "Epoch: 77/5000 \t Mean Square Error Loss: 0.5252046605034353\n",
      "Epoch: 78/5000 \t Mean Square Error Loss: 0.519060522941366\n",
      "Epoch: 79/5000 \t Mean Square Error Loss: 0.5141386008162877\n",
      "Epoch: 80/5000 \t Mean Square Error Loss: 0.5095482381317905\n",
      "Epoch: 81/5000 \t Mean Square Error Loss: 0.5048310427486148\n",
      "Epoch: 82/5000 \t Mean Square Error Loss: 0.5043229548003384\n",
      "Epoch: 83/5000 \t Mean Square Error Loss: 0.5028971548359763\n",
      "Epoch: 84/5000 \t Mean Square Error Loss: 0.49765721624366405\n",
      "Epoch: 85/5000 \t Mean Square Error Loss: 0.490060860143047\n",
      "Epoch: 86/5000 \t Mean Square Error Loss: 0.48510113720115766\n",
      "Epoch: 87/5000 \t Mean Square Error Loss: 0.4796525803569969\n",
      "Epoch: 88/5000 \t Mean Square Error Loss: 0.46928859355559405\n",
      "Epoch: 89/5000 \t Mean Square Error Loss: 0.4633650430575575\n",
      "Epoch: 90/5000 \t Mean Square Error Loss: 0.4592166405841397\n",
      "Epoch: 91/5000 \t Mean Square Error Loss: 0.4685361245686041\n",
      "Epoch: 92/5000 \t Mean Square Error Loss: 0.45621801919019367\n",
      "Epoch: 93/5000 \t Mean Square Error Loss: 0.4478267476149683\n",
      "Epoch: 94/5000 \t Mean Square Error Loss: 0.44425194333287954\n",
      "Epoch: 95/5000 \t Mean Square Error Loss: 0.44510376004494384\n",
      "Epoch: 96/5000 \t Mean Square Error Loss: 0.44463248831457675\n",
      "Epoch: 97/5000 \t Mean Square Error Loss: 0.4401201972402788\n",
      "Epoch: 98/5000 \t Mean Square Error Loss: 0.4327042322278522\n",
      "Epoch: 99/5000 \t Mean Square Error Loss: 0.4261007368813998\n",
      "Epoch: 100/5000 \t Mean Square Error Loss: 0.41998687398982343\n",
      "Epoch: 101/5000 \t Mean Square Error Loss: 0.4139773581317278\n",
      "Epoch: 102/5000 \t Mean Square Error Loss: 0.40907704830169683\n",
      "Epoch: 103/5000 \t Mean Square Error Loss: 0.404094829718937\n",
      "Epoch: 104/5000 \t Mean Square Error Loss: 0.40134838435440384\n",
      "Epoch: 105/5000 \t Mean Square Error Loss: 0.39945195758691887\n",
      "Epoch: 106/5000 \t Mean Square Error Loss: 0.4014670484734379\n",
      "Epoch: 107/5000 \t Mean Square Error Loss: 0.39781790797181715\n",
      "Epoch: 108/5000 \t Mean Square Error Loss: 0.39786613087275036\n",
      "Epoch: 109/5000 \t Mean Square Error Loss: 0.40250836555927866\n",
      "Epoch: 110/5000 \t Mean Square Error Loss: 0.39595461639899093\n",
      "Epoch: 111/5000 \t Mean Square Error Loss: 0.3932360425653817\n",
      "Epoch: 112/5000 \t Mean Square Error Loss: 0.38507393513763305\n",
      "Epoch: 113/5000 \t Mean Square Error Loss: 0.3809445792162268\n",
      "Epoch: 114/5000 \t Mean Square Error Loss: 0.3770601554894547\n",
      "Epoch: 115/5000 \t Mean Square Error Loss: 0.378424780638148\n",
      "Epoch: 116/5000 \t Mean Square Error Loss: 0.37482868128740643\n",
      "Epoch: 117/5000 \t Mean Square Error Loss: 0.37511243231625735\n",
      "Epoch: 118/5000 \t Mean Square Error Loss: 0.3715502158368482\n",
      "Epoch: 119/5000 \t Mean Square Error Loss: 0.37278488789642206\n",
      "Epoch: 120/5000 \t Mean Square Error Loss: 0.3689619078296996\n",
      "Epoch: 121/5000 \t Mean Square Error Loss: 0.36623092974578986\n",
      "Epoch: 122/5000 \t Mean Square Error Loss: 0.36546491329640024\n",
      "Epoch: 123/5000 \t Mean Square Error Loss: 0.3618142525022499\n",
      "Epoch: 124/5000 \t Mean Square Error Loss: 0.3593123383103055\n",
      "Epoch: 125/5000 \t Mean Square Error Loss: 0.3607110528267578\n",
      "Epoch: 126/5000 \t Mean Square Error Loss: 0.3625608722535137\n",
      "Epoch: 127/5000 \t Mean Square Error Loss: 0.3609673917044157\n",
      "Epoch: 128/5000 \t Mean Square Error Loss: 0.3553050452196448\n",
      "Epoch: 129/5000 \t Mean Square Error Loss: 0.34826713725612757\n",
      "Epoch: 130/5000 \t Mean Square Error Loss: 0.3503379582361198\n",
      "Epoch: 131/5000 \t Mean Square Error Loss: 0.3494395316894085\n",
      "Epoch: 132/5000 \t Mean Square Error Loss: 0.35176476723978206\n",
      "Epoch: 133/5000 \t Mean Square Error Loss: 0.3498192990674135\n",
      "Epoch: 134/5000 \t Mean Square Error Loss: 0.34536343887760046\n",
      "Epoch: 135/5000 \t Mean Square Error Loss: 0.34137970433574344\n",
      "Epoch: 136/5000 \t Mean Square Error Loss: 0.33955551041718807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137/5000 \t Mean Square Error Loss: 0.3427108341679912\n",
      "Epoch: 138/5000 \t Mean Square Error Loss: 0.3397007447406338\n",
      "Epoch: 139/5000 \t Mean Square Error Loss: 0.33660106180103255\n",
      "Epoch: 140/5000 \t Mean Square Error Loss: 0.3326295828719519\n",
      "Epoch: 141/5000 \t Mean Square Error Loss: 0.3271501842403013\n",
      "Epoch: 142/5000 \t Mean Square Error Loss: 0.32802951485542076\n",
      "Epoch: 143/5000 \t Mean Square Error Loss: 0.33045122563589563\n",
      "Epoch: 144/5000 \t Mean Square Error Loss: 0.3304753313503504\n",
      "Epoch: 145/5000 \t Mean Square Error Loss: 0.3284201691838987\n",
      "Epoch: 146/5000 \t Mean Square Error Loss: 0.33023042359611476\n",
      "Epoch: 147/5000 \t Mean Square Error Loss: 0.32898017951135355\n",
      "Epoch: 148/5000 \t Mean Square Error Loss: 0.3248165377014352\n",
      "Epoch: 149/5000 \t Mean Square Error Loss: 0.3195760334884772\n",
      "Epoch: 150/5000 \t Mean Square Error Loss: 0.31320617388481864\n",
      "Epoch: 151/5000 \t Mean Square Error Loss: 0.31389368129075823\n",
      "Epoch: 152/5000 \t Mean Square Error Loss: 0.31488009127612887\n",
      "Epoch: 153/5000 \t Mean Square Error Loss: 0.3176264443656889\n",
      "Epoch: 154/5000 \t Mean Square Error Loss: 0.3199565251003249\n",
      "Epoch: 155/5000 \t Mean Square Error Loss: 0.3273407774490293\n",
      "Epoch: 156/5000 \t Mean Square Error Loss: 0.3204967007976197\n",
      "Epoch: 157/5000 \t Mean Square Error Loss: 0.32633715892935394\n",
      "Epoch: 158/5000 \t Mean Square Error Loss: 0.3161205382526668\n",
      "Epoch: 159/5000 \t Mean Square Error Loss: 0.3117597741561954\n",
      "Epoch: 160/5000 \t Mean Square Error Loss: 0.31094310772468853\n",
      "Epoch: 161/5000 \t Mean Square Error Loss: 0.3114431213634283\n",
      "Epoch: 162/5000 \t Mean Square Error Loss: 0.3115364412882338\n",
      "Epoch: 163/5000 \t Mean Square Error Loss: 0.32465203337090787\n",
      "Epoch: 164/5000 \t Mean Square Error Loss: 0.32767886048081535\n",
      "Epoch: 165/5000 \t Mean Square Error Loss: 0.31372809709365396\n",
      "Epoch: 166/5000 \t Mean Square Error Loss: 0.31050881331934593\n",
      "Epoch: 167/5000 \t Mean Square Error Loss: 0.308488879243699\n",
      "Epoch: 168/5000 \t Mean Square Error Loss: 0.3092206851209057\n",
      "Epoch: 169/5000 \t Mean Square Error Loss: 0.3075075398927951\n",
      "Epoch: 170/5000 \t Mean Square Error Loss: 0.30514385859836596\n",
      "Epoch: 171/5000 \t Mean Square Error Loss: 0.30681606965084945\n",
      "Epoch: 172/5000 \t Mean Square Error Loss: 0.30656556803811047\n",
      "Epoch: 173/5000 \t Mean Square Error Loss: 0.3040725591292441\n",
      "Epoch: 174/5000 \t Mean Square Error Loss: 0.30266936802963834\n",
      "Epoch: 175/5000 \t Mean Square Error Loss: 0.3053249504775681\n",
      "Epoch: 176/5000 \t Mean Square Error Loss: 0.3083693392605961\n",
      "Epoch: 177/5000 \t Mean Square Error Loss: 0.3070073771177476\n",
      "Epoch: 178/5000 \t Mean Square Error Loss: 0.3062633555304555\n",
      "Epoch: 179/5000 \t Mean Square Error Loss: 0.3056377042786346\n",
      "Epoch: 180/5000 \t Mean Square Error Loss: 0.29984505455863025\n",
      "Epoch: 181/5000 \t Mean Square Error Loss: 0.2984435224134055\n",
      "Epoch: 182/5000 \t Mean Square Error Loss: 0.2985927335387992\n",
      "Epoch: 183/5000 \t Mean Square Error Loss: 0.2958094265670458\n",
      "Epoch: 184/5000 \t Mean Square Error Loss: 0.2945387064163655\n",
      "Epoch: 185/5000 \t Mean Square Error Loss: 0.2953737984142542\n",
      "Epoch: 186/5000 \t Mean Square Error Loss: 0.29654486608305736\n",
      "Epoch: 187/5000 \t Mean Square Error Loss: 0.29645380016151335\n",
      "Epoch: 188/5000 \t Mean Square Error Loss: 0.29786160002193696\n",
      "Epoch: 189/5000 \t Mean Square Error Loss: 0.2934676154388045\n",
      "Epoch: 190/5000 \t Mean Square Error Loss: 0.2946968482628028\n",
      "Epoch: 191/5000 \t Mean Square Error Loss: 0.2952247523862448\n",
      "Epoch: 192/5000 \t Mean Square Error Loss: 0.2979018049758847\n",
      "Epoch: 193/5000 \t Mean Square Error Loss: 0.3014132612419927\n",
      "Epoch: 194/5000 \t Mean Square Error Loss: 0.29800237422208903\n",
      "Epoch: 195/5000 \t Mean Square Error Loss: 0.2956467023953234\n",
      "Epoch: 196/5000 \t Mean Square Error Loss: 0.29169456878965366\n",
      "Epoch: 197/5000 \t Mean Square Error Loss: 0.29579831516393557\n",
      "Epoch: 198/5000 \t Mean Square Error Loss: 0.29304885465230907\n",
      "Epoch: 199/5000 \t Mean Square Error Loss: 0.2932515478533182\n",
      "Epoch: 200/5000 \t Mean Square Error Loss: 0.29519156581687134\n",
      "Epoch: 201/5000 \t Mean Square Error Loss: 0.290343850726363\n",
      "Epoch: 202/5000 \t Mean Square Error Loss: 0.28963380777686204\n",
      "Epoch: 203/5000 \t Mean Square Error Loss: 0.2869155766076123\n",
      "Epoch: 204/5000 \t Mean Square Error Loss: 0.2859301896274838\n",
      "Epoch: 205/5000 \t Mean Square Error Loss: 0.28776887121559697\n",
      "Epoch: 206/5000 \t Mean Square Error Loss: 0.29233964965932036\n",
      "Epoch: 207/5000 \t Mean Square Error Loss: 0.28857031477046313\n",
      "Epoch: 208/5000 \t Mean Square Error Loss: 0.28737493339442804\n",
      "Epoch: 209/5000 \t Mean Square Error Loss: 0.2885432218407986\n",
      "Epoch: 210/5000 \t Mean Square Error Loss: 0.28591276013202743\n",
      "Epoch: 211/5000 \t Mean Square Error Loss: 0.2874493374485351\n",
      "Epoch: 212/5000 \t Mean Square Error Loss: 0.2861101542556635\n",
      "Epoch: 213/5000 \t Mean Square Error Loss: 0.2840290338923243\n",
      "Epoch: 214/5000 \t Mean Square Error Loss: 0.2831488488608324\n",
      "Epoch: 215/5000 \t Mean Square Error Loss: 0.28073974533559887\n",
      "Epoch: 216/5000 \t Mean Square Error Loss: 0.2796151493383751\n",
      "Epoch: 217/5000 \t Mean Square Error Loss: 0.27914596701266875\n",
      "Epoch: 218/5000 \t Mean Square Error Loss: 0.2788720909022886\n",
      "Epoch: 219/5000 \t Mean Square Error Loss: 0.27778751082001374\n",
      "Epoch: 220/5000 \t Mean Square Error Loss: 0.2797473579271068\n",
      "Epoch: 221/5000 \t Mean Square Error Loss: 0.2830107950266435\n",
      "Epoch: 222/5000 \t Mean Square Error Loss: 0.2829846707348045\n",
      "Epoch: 223/5000 \t Mean Square Error Loss: 0.2842643789666467\n",
      "Epoch: 224/5000 \t Mean Square Error Loss: 0.28496208949069096\n",
      "Epoch: 225/5000 \t Mean Square Error Loss: 0.28590380297545104\n",
      "Epoch: 226/5000 \t Mean Square Error Loss: 0.2879120590297747\n",
      "Epoch: 227/5000 \t Mean Square Error Loss: 0.2921201908438774\n",
      "Epoch: 228/5000 \t Mean Square Error Loss: 0.28907831343646834\n",
      "Epoch: 229/5000 \t Mean Square Error Loss: 0.2871516119984902\n",
      "Epoch: 230/5000 \t Mean Square Error Loss: 0.2863496546964764\n",
      "Epoch: 231/5000 \t Mean Square Error Loss: 0.283713949275316\n",
      "Epoch: 232/5000 \t Mean Square Error Loss: 0.28343772489156693\n",
      "Epoch: 233/5000 \t Mean Square Error Loss: 0.2808868620684955\n",
      "Epoch: 234/5000 \t Mean Square Error Loss: 0.27961181097948407\n",
      "Epoch: 235/5000 \t Mean Square Error Loss: 0.27932142064162385\n",
      "Epoch: 236/5000 \t Mean Square Error Loss: 0.2794849707990511\n",
      "Epoch: 237/5000 \t Mean Square Error Loss: 0.2792351684809729\n",
      "Epoch: 238/5000 \t Mean Square Error Loss: 0.2782533692515545\n",
      "Epoch: 239/5000 \t Mean Square Error Loss: 0.2799803632073822\n",
      "Epoch: 240/5000 \t Mean Square Error Loss: 0.2786139593962346\n",
      "Epoch: 241/5000 \t Mean Square Error Loss: 0.2788814999568413\n",
      "Epoch: 242/5000 \t Mean Square Error Loss: 0.277681657958729\n",
      "Epoch: 243/5000 \t Mean Square Error Loss: 0.2772351333785755\n",
      "Epoch: 244/5000 \t Mean Square Error Loss: 0.2789055059145685\n",
      "Epoch: 245/5000 \t Mean Square Error Loss: 0.27945082656509207\n",
      "Epoch: 246/5000 \t Mean Square Error Loss: 0.2792229073815765\n",
      "Epoch: 247/5000 \t Mean Square Error Loss: 0.2776707730033906\n",
      "Epoch: 248/5000 \t Mean Square Error Loss: 0.2770491364610743\n",
      "Epoch: 249/5000 \t Mean Square Error Loss: 0.27658285955006107\n",
      "Epoch: 250/5000 \t Mean Square Error Loss: 0.27910134931987296\n",
      "Epoch: 251/5000 \t Mean Square Error Loss: 0.27813664190938786\n",
      "Epoch: 252/5000 \t Mean Square Error Loss: 0.2755633817058228\n",
      "Epoch: 253/5000 \t Mean Square Error Loss: 0.2748098418303614\n",
      "Epoch: 254/5000 \t Mean Square Error Loss: 0.2751842408000675\n",
      "Epoch: 255/5000 \t Mean Square Error Loss: 0.2770499594540775\n",
      "Epoch: 256/5000 \t Mean Square Error Loss: 0.27752796027450877\n",
      "Epoch: 257/5000 \t Mean Square Error Loss: 0.2764096080508691\n",
      "Epoch: 258/5000 \t Mean Square Error Loss: 0.2768201384085492\n",
      "Epoch: 259/5000 \t Mean Square Error Loss: 0.2770326067713015\n",
      "Epoch: 260/5000 \t Mean Square Error Loss: 0.2755612279580727\n",
      "Epoch: 261/5000 \t Mean Square Error Loss: 0.27688158855278616\n",
      "Epoch: 262/5000 \t Mean Square Error Loss: 0.27765906705018367\n",
      "Epoch: 263/5000 \t Mean Square Error Loss: 0.27853765098619665\n",
      "Epoch: 264/5000 \t Mean Square Error Loss: 0.277790218716386\n",
      "Epoch: 265/5000 \t Mean Square Error Loss: 0.2768712148506771\n",
      "Epoch: 266/5000 \t Mean Square Error Loss: 0.2773921135579193\n",
      "Epoch: 267/5000 \t Mean Square Error Loss: 0.27641071136027695\n",
      "Epoch: 268/5000 \t Mean Square Error Loss: 0.2770944160397581\n",
      "Epoch: 269/5000 \t Mean Square Error Loss: 0.2765123559840055\n",
      "Epoch: 270/5000 \t Mean Square Error Loss: 0.2763299313549217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 271/5000 \t Mean Square Error Loss: 0.27603604604010806\n",
      "Epoch: 272/5000 \t Mean Square Error Loss: 0.2754497687686936\n",
      "Epoch: 273/5000 \t Mean Square Error Loss: 0.27463351183855383\n",
      "Epoch: 274/5000 \t Mean Square Error Loss: 0.27405860434017415\n",
      "Epoch: 275/5000 \t Mean Square Error Loss: 0.27469500188548196\n",
      "Epoch: 276/5000 \t Mean Square Error Loss: 0.2754092974642829\n",
      "Epoch: 277/5000 \t Mean Square Error Loss: 0.273670134684032\n",
      "Epoch: 278/5000 \t Mean Square Error Loss: 0.2724295485468589\n",
      "Epoch: 279/5000 \t Mean Square Error Loss: 0.2722752747675365\n",
      "Epoch: 280/5000 \t Mean Square Error Loss: 0.27175003564507394\n",
      "Epoch: 281/5000 \t Mean Square Error Loss: 0.2747773989473926\n",
      "Epoch: 282/5000 \t Mean Square Error Loss: 0.2767120470062958\n",
      "Epoch: 283/5000 \t Mean Square Error Loss: 0.2756722133029954\n",
      "Epoch: 284/5000 \t Mean Square Error Loss: 0.2751406161854955\n",
      "Epoch: 285/5000 \t Mean Square Error Loss: 0.2751499409456133\n",
      "Epoch: 286/5000 \t Mean Square Error Loss: 0.2736848088986704\n",
      "Epoch: 287/5000 \t Mean Square Error Loss: 0.27493409132857705\n",
      "Epoch: 288/5000 \t Mean Square Error Loss: 0.2737602354592359\n",
      "Epoch: 289/5000 \t Mean Square Error Loss: 0.2730657291212841\n",
      "Epoch: 290/5000 \t Mean Square Error Loss: 0.2731796408298125\n",
      "Epoch: 291/5000 \t Mean Square Error Loss: 0.27225804877580456\n",
      "Epoch: 292/5000 \t Mean Square Error Loss: 0.27168140850306555\n",
      "Epoch: 293/5000 \t Mean Square Error Loss: 0.2712692885219303\n",
      "Epoch: 294/5000 \t Mean Square Error Loss: 0.27203694048286986\n",
      "Epoch: 295/5000 \t Mean Square Error Loss: 0.27149295757006403\n",
      "Epoch: 296/5000 \t Mean Square Error Loss: 0.2735351089653111\n",
      "Epoch: 297/5000 \t Mean Square Error Loss: 0.27266088739099864\n",
      "Epoch: 298/5000 \t Mean Square Error Loss: 0.2719710626362757\n",
      "Epoch: 299/5000 \t Mean Square Error Loss: 0.2727529837995394\n",
      "Epoch: 300/5000 \t Mean Square Error Loss: 0.27261373687488766\n",
      "Epoch: 301/5000 \t Mean Square Error Loss: 0.27344597832428363\n",
      "Epoch: 302/5000 \t Mean Square Error Loss: 0.27374468587931233\n",
      "Epoch: 303/5000 \t Mean Square Error Loss: 0.27361122933391746\n",
      "Epoch: 304/5000 \t Mean Square Error Loss: 0.2749392672063915\n",
      "Epoch: 305/5000 \t Mean Square Error Loss: 0.27586931513941937\n",
      "Epoch: 306/5000 \t Mean Square Error Loss: 0.2739627291467898\n",
      "Epoch: 307/5000 \t Mean Square Error Loss: 0.27213603133437025\n",
      "Epoch: 308/5000 \t Mean Square Error Loss: 0.27048730501071183\n",
      "Epoch: 309/5000 \t Mean Square Error Loss: 0.2722769980650068\n",
      "Epoch: 310/5000 \t Mean Square Error Loss: 0.27147279174756794\n",
      "Epoch: 311/5000 \t Mean Square Error Loss: 0.2704171306418574\n",
      "Epoch: 312/5000 \t Mean Square Error Loss: 0.2688454599061272\n",
      "Epoch: 313/5000 \t Mean Square Error Loss: 0.2692292983561879\n",
      "Epoch: 314/5000 \t Mean Square Error Loss: 0.2687929714075192\n",
      "Epoch: 315/5000 \t Mean Square Error Loss: 0.2698190107505192\n",
      "Epoch: 316/5000 \t Mean Square Error Loss: 0.2711923596250462\n",
      "Epoch: 317/5000 \t Mean Square Error Loss: 0.2723447209122789\n",
      "Epoch: 318/5000 \t Mean Square Error Loss: 0.27400760121924106\n",
      "Epoch: 319/5000 \t Mean Square Error Loss: 0.27354213632799096\n",
      "Epoch: 320/5000 \t Mean Square Error Loss: 0.27264127172685565\n",
      "Epoch: 321/5000 \t Mean Square Error Loss: 0.27220574931619557\n",
      "Epoch: 322/5000 \t Mean Square Error Loss: 0.2725365391336226\n",
      "Epoch: 323/5000 \t Mean Square Error Loss: 0.2722797423725846\n",
      "Epoch: 324/5000 \t Mean Square Error Loss: 0.2738246094731606\n",
      "Epoch: 325/5000 \t Mean Square Error Loss: 0.2729161942853089\n",
      "Epoch: 326/5000 \t Mean Square Error Loss: 0.2727172449542888\n",
      "Epoch: 327/5000 \t Mean Square Error Loss: 0.27262517548006454\n",
      "Epoch: 328/5000 \t Mean Square Error Loss: 0.27171055093469976\n",
      "Epoch: 329/5000 \t Mean Square Error Loss: 0.2714997081078246\n",
      "Epoch: 330/5000 \t Mean Square Error Loss: 0.27033936977386475\n",
      "Epoch: 331/5000 \t Mean Square Error Loss: 0.2695486740088364\n",
      "Epoch: 332/5000 \t Mean Square Error Loss: 0.26902440202784833\n",
      "Epoch: 333/5000 \t Mean Square Error Loss: 0.2688891862725613\n",
      "Epoch: 334/5000 \t Mean Square Error Loss: 0.26948420871750584\n",
      "Epoch: 335/5000 \t Mean Square Error Loss: 0.26982103930357615\n",
      "Epoch: 336/5000 \t Mean Square Error Loss: 0.2694266635503729\n",
      "Epoch: 337/5000 \t Mean Square Error Loss: 0.2703667270588575\n",
      "Epoch: 338/5000 \t Mean Square Error Loss: 0.27145874699788114\n",
      "Epoch: 339/5000 \t Mean Square Error Loss: 0.2704200764580252\n",
      "Epoch: 340/5000 \t Mean Square Error Loss: 0.2709195843301557\n",
      "Epoch: 341/5000 \t Mean Square Error Loss: 0.27372425470392076\n",
      "Epoch: 342/5000 \t Mean Square Error Loss: 0.2737717942712695\n",
      "Epoch: 343/5000 \t Mean Square Error Loss: 0.27258865803355453\n",
      "Epoch: 344/5000 \t Mean Square Error Loss: 0.2721123635519498\n",
      "Epoch: 345/5000 \t Mean Square Error Loss: 0.2726035851314976\n",
      "Epoch: 346/5000 \t Mean Square Error Loss: 0.27131679866104447\n",
      "Epoch: 347/5000 \t Mean Square Error Loss: 0.2701527234400666\n",
      "Epoch: 348/5000 \t Mean Square Error Loss: 0.2696266323951498\n",
      "Epoch: 349/5000 \t Mean Square Error Loss: 0.2705281548919039\n",
      "Epoch: 350/5000 \t Mean Square Error Loss: 0.2702589922869056\n",
      "Epoch: 351/5000 \t Mean Square Error Loss: 0.27233590690660675\n",
      "Epoch: 352/5000 \t Mean Square Error Loss: 0.2738239660422672\n",
      "Epoch: 353/5000 \t Mean Square Error Loss: 0.27378813143055813\n",
      "Epoch: 354/5000 \t Mean Square Error Loss: 0.27211690846846187\n",
      "Epoch: 355/5000 \t Mean Square Error Loss: 0.2719156956572912\n",
      "Epoch: 356/5000 \t Mean Square Error Loss: 0.2708192450232087\n",
      "Epoch: 357/5000 \t Mean Square Error Loss: 0.2723669846187576\n",
      "Epoch: 358/5000 \t Mean Square Error Loss: 0.2741587630874442\n",
      "Epoch: 359/5000 \t Mean Square Error Loss: 0.27149600663444484\n",
      "Epoch: 360/5000 \t Mean Square Error Loss: 0.2700778929259488\n",
      "Epoch: 361/5000 \t Mean Square Error Loss: 0.26907513929710236\n",
      "Epoch: 362/5000 \t Mean Square Error Loss: 0.2699980780669336\n",
      "Epoch: 363/5000 \t Mean Square Error Loss: 0.2703368509164914\n",
      "Epoch: 364/5000 \t Mean Square Error Loss: 0.27052213806487524\n",
      "Epoch: 365/5000 \t Mean Square Error Loss: 0.269033019512767\n",
      "Epoch: 366/5000 \t Mean Square Error Loss: 0.26966283710431843\n",
      "Epoch: 367/5000 \t Mean Square Error Loss: 0.270575574252396\n",
      "Epoch: 368/5000 \t Mean Square Error Loss: 0.26989371457359285\n",
      "Epoch: 369/5000 \t Mean Square Error Loss: 0.2690220587423157\n",
      "Epoch: 370/5000 \t Mean Square Error Loss: 0.26897079026848697\n",
      "Epoch: 371/5000 \t Mean Square Error Loss: 0.2693120520483998\n",
      "Epoch: 372/5000 \t Mean Square Error Loss: 0.2691257084762701\n",
      "Epoch: 373/5000 \t Mean Square Error Loss: 0.26839680502105456\n",
      "Epoch: 374/5000 \t Mean Square Error Loss: 0.26861372802048045\n",
      "Epoch: 375/5000 \t Mean Square Error Loss: 0.26887293789676037\n",
      "Epoch: 376/5000 \t Mean Square Error Loss: 0.2692997121411886\n",
      "Epoch: 377/5000 \t Mean Square Error Loss: 0.26908113617278534\n",
      "Epoch: 378/5000 \t Mean Square Error Loss: 0.2689797913180235\n",
      "Epoch: 379/5000 \t Mean Square Error Loss: 0.2692547537791679\n",
      "Epoch: 380/5000 \t Mean Square Error Loss: 0.26817049241963786\n",
      "Epoch: 381/5000 \t Mean Square Error Loss: 0.26863703967138314\n",
      "Epoch: 382/5000 \t Mean Square Error Loss: 0.26937488930993503\n",
      "Epoch: 383/5000 \t Mean Square Error Loss: 0.2701218063362473\n",
      "Epoch: 384/5000 \t Mean Square Error Loss: 0.27139708487059777\n",
      "Epoch: 385/5000 \t Mean Square Error Loss: 0.2721026392661378\n",
      "Epoch: 386/5000 \t Mean Square Error Loss: 0.273308879660762\n",
      "Epoch: 387/5000 \t Mean Square Error Loss: 0.27216825924159094\n",
      "Epoch: 388/5000 \t Mean Square Error Loss: 0.2711548047085686\n",
      "Epoch: 389/5000 \t Mean Square Error Loss: 0.2707104468445398\n",
      "Epoch: 390/5000 \t Mean Square Error Loss: 0.26949926300527666\n",
      "Epoch: 391/5000 \t Mean Square Error Loss: 0.2693804582292565\n",
      "Epoch: 392/5000 \t Mean Square Error Loss: 0.2689116729353262\n",
      "Epoch: 393/5000 \t Mean Square Error Loss: 0.2688465452593241\n",
      "Epoch: 394/5000 \t Mean Square Error Loss: 0.2695569727710101\n",
      "Epoch: 395/5000 \t Mean Square Error Loss: 0.26942389081212764\n",
      "Epoch: 396/5000 \t Mean Square Error Loss: 0.2692193306639604\n",
      "Epoch: 397/5000 \t Mean Square Error Loss: 0.27018502167578023\n",
      "Epoch: 398/5000 \t Mean Square Error Loss: 0.2705132562246283\n",
      "Epoch: 399/5000 \t Mean Square Error Loss: 0.26986843272731886\n",
      "Epoch: 400/5000 \t Mean Square Error Loss: 0.26997204355614945\n",
      "Epoch: 401/5000 \t Mean Square Error Loss: 0.27007533017561525\n",
      "Epoch: 402/5000 \t Mean Square Error Loss: 0.2695343180181591\n",
      "Epoch: 403/5000 \t Mean Square Error Loss: 0.26840048903700703\n",
      "Epoch: 404/5000 \t Mean Square Error Loss: 0.2697850440835354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 405/5000 \t Mean Square Error Loss: 0.26901109996699885\n",
      "Epoch: 406/5000 \t Mean Square Error Loss: 0.2693834658945954\n",
      "Epoch: 407/5000 \t Mean Square Error Loss: 0.2700259999750049\n"
     ]
    }
   ],
   "source": [
    "# training and save results\n",
    "trainloss = []\n",
    "for epoch in range(epochs):\n",
    "    training_loss = 0\n",
    "\n",
    "    for X_train, Y_train in trainloader:\n",
    "        X_train = X_train.view(-1, X_train.shape[-1], X_train.shape[-3], X_train.shape[-2])\n",
    "        Y_train = Y_train.view(-1, Y_train.shape[-1], Y_train.shape[-3], Y_train.shape[-2])\n",
    "\n",
    "        if GPU:\n",
    "            X_train, Y_train = Variable(X_train).cuda(), Variable(Y_train).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        X_pred = net(X_train)\n",
    "        loss = criterion(X_pred, Y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item() / X_train.shape[0]\n",
    "\n",
    "    print('Epoch: {}/{} \\t Mean Square Error Loss: {}'.format(epoch + 1, epochs, training_loss))\n",
    "    trainloss.append(training_loss)\n",
    "\n",
    "np.save('{}/Trainloss_Epochs_{}_Horizon_{}_Smooth_{}.npy'\n",
    "            .format(save_path, epochs, horizon, smoothing), np.asarray(trainloss))\n",
    "\n",
    "# save trained model as pth\n",
    "torch.save(net.state_dict(), '{}/AE_Epochs_{}_Horizon_{}_Smooth_{}.pth'\n",
    "               .format(save_path, epochs, horizon, smoothing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"MSE During Training\")\n",
    "plt.plot(trainloss[100:], label=\"nf={}\".format(nf))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig('{}/Trainloss_Epochs_{}_Horizon_{}_Smooth_{}.png'\n",
    "                .format(save_path, epochs, horizon, smoothing), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on testing dataset\n",
    "X_test, Y_test = next(iter(testloader))\n",
    "X_test = X_test.view(-1, X_test.shape[-1], X_test.shape[-3], X_test.shape[-2])\n",
    "Y_test = Y_test.view(-1, Y_test.shape[-1], Y_test.shape[-3], Y_test.shape[-2])\n",
    "if GPU:\n",
    "    X_test = Variable(X_test).cuda()\n",
    "    Y_pred = net(X_test)\n",
    "\n",
    "for num in range(Y_test.shape[0]):\n",
    "    fig = plt.figure()\n",
    "    plot = fig.add_subplot(1, 2, 1)\n",
    "    plot.set_title('Original Image')\n",
    "    imgplot = plt.imshow(Y_test[num, 0, :, :].cpu(), origin='lower', cmap='Blues')\n",
    "\n",
    "    plot = fig.add_subplot(1, 2, 2)\n",
    "    plot.set_title('Generated Image')\n",
    "    imgplot = plt.imshow(Y_pred[num, 0, :, :].cpu().detach(), origin='lower', cmap='Blues')\n",
    "\n",
    "    plt.savefig('{}/AE_{}.png'.format(save_path, num), dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
